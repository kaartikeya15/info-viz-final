{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEOISouQFR7X",
        "outputId": "fdb75944-462b-4dc7-d619-f196bdf5d184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraped: Scott Armstrong\n",
            "Scraped: Marshall Ball\n",
            "Scraped: Gerard BEN AROUS\n",
            "Scraped: Nir Bitansky\n",
            "Scraped: Fedor Bogomolov\n",
            "Scraped: Joseph Bonneau\n",
            "Scraped: Joan Bruna\n",
            "Scraped: Tristan Buckmaster\n",
            "Scraped: Yu Chen\n",
            "Scraped: Kyunghyun Cho\n",
            "Scraped: Eunsol Choi\n",
            "Scraped: Sumit Chopra\n",
            "Scraped: Richard Cole\n",
            "Scraped: Patrick Cousot\n",
            "Scraped: David Fouhey\n",
            "Scraped: Anupam Gupta\n",
            "Scraped: Ernest Davis\n",
            "Scraped: percy deift\n",
            "Scraped: percy deift\n",
            "Scraped: Yevgeniy Dodis\n",
            "Scraped: Yevgeniy Dodis\n",
            "Scraped: Rob Fergus\n",
            "Scraped: Carlos Fernandez-Granda\n",
            "Scraped: Gilles Francfort\n",
            "Scraped: Alfred Galichon\n",
            "Scraped: Davi Geiger\n",
            "Scraped: Edwin P. Gerber\n",
            "Scraped: Leslie Greengard\n",
            "Scraped: Yanjun Han\n",
            "Scraped: Arthur Jacot\n",
            "Scraped: Qi Lei\n",
            "Scraped: Fengbo Hang\n",
            "Scraped: Nina Holden\n",
            "Scraped: David M Holland\n",
            "Scraped: Alan Ali Kaptanoglu\n",
            "Scraped: Julia Kempe\n",
            "Scraped: Yann LeCun\n",
            "Scraped: Chao Li\n",
            "Scraped: Jinyang Li\n",
            "Scraped: Eyal Lubetzky\n",
            "Scraped: Nader Masmoudi\n",
            "Scraped: Bud Mishra\n",
            "Scraped: Alex Mogilner\n",
            "Scraped: Mehryar Mohri\n",
            "Scraped: Sunoo Park\n",
            "Scraped: Jean Ponce\n",
            "Scraped: Michael O'Neil\n",
            "Scraped: Michael L. Overton\n",
            "Scraped: Aurojit Panda\n",
            "Scraped: Daniele Panozzo\n",
            "Scraped: Olivier Pauluis\n",
            "Scraped: Benjamin Peherstorfer\n",
            "Scraped: Ken Perlin\n",
            "Scraped: Lerrel Pinto\n",
            "Scraped: Jonathan Niles-Weed\n",
            "Scraped: Peter Unger\n",
            "Scraped: Victor Shoup\n",
            "Scraped: Denis Zorin\n",
            "Scraped: Yi Zhang（张祎）\n",
            "Scraped: Sai Qian Zhang\n",
            "Scraped: Jun Zhang, 張駿\n",
            "Scraped: Gaoyong Zhang\n",
            "Scraped: ofer zeitouni\n",
            "Scraped: Laure Zanna\n",
            "Scraped: Robert J.C. Young\n",
            "Scraped: Chee Yap\n",
            "Scraped: Yisong Yang\n",
            "Scraped: Sherry Yang\n",
            "Scraped: Deane Yang\n",
            "Scraped: Hau-Tieng Wu\n",
            "Scraped: Saining Xie\n",
            "Scraped: Andrew Gordon Wilson\n",
            "Scraped: Sam Westrick\n",
            "Scraped: Hong Wang\n",
            "Scraped: Michael Walfish\n",
            "Scraped: Vlad Vicol\n",
            "Scraped: Eric Vanden-Eijnden\n",
            "Scraped: Matus Telgarsky\n",
            "Scraped: Joseph Tassarotti\n",
            "Scraped: Rajesh Ranganath\n",
            "Scraped: Theodore (Ted) S. Rappaport\n",
            "Scraped: Oded Regev\n",
            "Scraped: John Rinzel\n",
            "Scraped: Leif Ristroph\n",
            "Scraped: Sylvia Serfaty\n",
            "Scraped: Jalal Shatah\n",
            "Scraped: Michael Shelley\n",
            "Scraping completed. Results saved to 'scraped_scholar_profiles.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Read the CSV file with Google Scholar links\n",
        "df = pd.read_csv('/content/NYU_CIMS_professors_links.csv', header=None, names=['url'])\n",
        "\n",
        "# Function to scrape profile data\n",
        "def scrape_profile(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    name = soup.find('div', id='gsc_prf_in').text if soup.find('div', id='gsc_prf_in') else 'N/A'\n",
        "    affiliation = soup.find('div', class_='gsc_prf_il').text if soup.find('div', class_='gsc_prf_il') else 'N/A'\n",
        "\n",
        "    citations = soup.find('td', class_='gsc_rsb_std')\n",
        "    total_citations = citations.text if citations else 'N/A'\n",
        "\n",
        "    h_index = soup.find_all('td', class_='gsc_rsb_std')[2]\n",
        "    h_index = h_index.text if h_index else 'N/A'\n",
        "\n",
        "    return {\n",
        "        'name': name,\n",
        "        'affiliation': affiliation,\n",
        "        'total_citations': total_citations,\n",
        "        'h_index': h_index,\n",
        "        'url': url\n",
        "    }\n",
        "\n",
        "# List to store scraped data\n",
        "scraped_data = []\n",
        "\n",
        "# Scrape each profile\n",
        "for url in df['url']:\n",
        "    try:\n",
        "        profile_data = scrape_profile(url)\n",
        "        scraped_data.append(profile_data)\n",
        "        print(f\"Scraped: {profile_data['name']}\")\n",
        "        time.sleep(random.uniform(2, 5))  # Random delay between requests\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {str(e)}\")\n",
        "\n",
        "# Create a DataFrame from scraped data\n",
        "result_df = pd.DataFrame(scraped_data)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "result_df.to_csv('scraped_scholar_profiles.csv', index=False)\n",
        "print(\"Scraping completed. Results saved to 'scraped_scholar_profiles.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SjzcBHrSUti",
        "outputId": "48d81b13-a8a5-4e15-f82d-9a99719b6a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraped research interests for: Scott Armstrong\n",
            "Scraped research interests for: Marshall Ball\n",
            "Scraped research interests for: Gerard BEN AROUS\n",
            "Scraped research interests for: Nir Bitansky\n",
            "Scraped research interests for: Fedor Bogomolov\n",
            "Scraped research interests for: Joseph Bonneau\n",
            "Scraped research interests for: Joan Bruna\n",
            "Scraped research interests for: Tristan Buckmaster\n",
            "Scraped research interests for: Yu Chen\n",
            "Scraped research interests for: Kyunghyun Cho\n",
            "Scraped research interests for: Eunsol Choi\n",
            "Scraped research interests for: Sumit Chopra\n",
            "Scraped research interests for: Richard Cole\n",
            "Scraped research interests for: Patrick Cousot\n",
            "Scraped research interests for: David Fouhey\n",
            "Scraped research interests for: Anupam Gupta\n",
            "Scraped research interests for: Ernest Davis\n",
            "Scraped research interests for: percy deift\n",
            "Scraped research interests for: percy deift\n",
            "Scraped research interests for: Yevgeniy Dodis\n",
            "Scraped research interests for: Yevgeniy Dodis\n",
            "Scraped research interests for: Rob Fergus\n",
            "Scraped research interests for: Carlos Fernandez-Granda\n",
            "Scraped research interests for: Gilles Francfort\n",
            "Scraped research interests for: Alfred Galichon\n",
            "Scraped research interests for: Davi Geiger\n",
            "Scraped research interests for: Edwin P. Gerber\n",
            "Scraped research interests for: Leslie Greengard\n",
            "Scraped research interests for: Yanjun Han\n",
            "Scraped research interests for: Arthur Jacot\n",
            "Scraped research interests for: Qi Lei\n",
            "Scraped research interests for: Fengbo Hang\n",
            "Scraped research interests for: Nina Holden\n",
            "Scraped research interests for: David M Holland\n",
            "Scraped research interests for: Alan Ali Kaptanoglu\n",
            "Scraped research interests for: Julia Kempe\n",
            "Scraped research interests for: Yann LeCun\n",
            "Scraped research interests for: Chao Li\n",
            "Scraped research interests for: Jinyang Li\n",
            "Scraped research interests for: Eyal Lubetzky\n",
            "Scraped research interests for: Nader Masmoudi\n",
            "Scraped research interests for: Bud Mishra\n",
            "Scraped research interests for: Alex Mogilner\n",
            "Scraped research interests for: Mehryar Mohri\n",
            "Scraped research interests for: Sunoo Park\n",
            "Scraped research interests for: Jean Ponce\n",
            "Scraped research interests for: Michael O'Neil\n",
            "Scraped research interests for: Michael L. Overton\n",
            "Scraped research interests for: Aurojit Panda\n",
            "Scraped research interests for: Daniele Panozzo\n",
            "Scraped research interests for: Olivier Pauluis\n",
            "Scraped research interests for: Benjamin Peherstorfer\n",
            "Scraped research interests for: Ken Perlin\n",
            "Scraped research interests for: Lerrel Pinto\n",
            "Scraped research interests for: Jonathan Niles-Weed\n",
            "Scraped research interests for: Peter Unger\n",
            "Scraped research interests for: Victor Shoup\n",
            "Scraped research interests for: Denis Zorin\n",
            "Scraped research interests for: Yi Zhang（张祎）\n",
            "Scraped research interests for: Sai Qian Zhang\n",
            "Scraped research interests for: Jun Zhang, 張駿\n",
            "Scraped research interests for: Gaoyong Zhang\n",
            "Scraped research interests for: ofer zeitouni\n",
            "Scraped research interests for: Laure Zanna\n",
            "Scraped research interests for: Robert J.C. Young\n",
            "Scraped research interests for: Chee Yap\n",
            "Scraped research interests for: Yisong Yang\n",
            "Scraped research interests for: Sherry Yang\n",
            "Scraped research interests for: Deane Yang\n",
            "Scraped research interests for: Hau-Tieng Wu\n",
            "Scraped research interests for: Saining Xie\n",
            "Scraped research interests for: Andrew Gordon Wilson\n",
            "Scraped research interests for: Sam Westrick\n",
            "Scraped research interests for: Hong Wang\n",
            "Scraped research interests for: Michael Walfish\n",
            "Scraped research interests for: Vlad Vicol\n",
            "Scraped research interests for: Eric Vanden-Eijnden\n",
            "Scraped research interests for: Matus Telgarsky\n",
            "Scraped research interests for: Joseph Tassarotti\n",
            "Scraped research interests for: Rajesh Ranganath\n",
            "Scraped research interests for: Theodore (Ted) S. Rappaport\n",
            "Scraped research interests for: Oded Regev\n",
            "Scraped research interests for: John Rinzel\n",
            "Scraped research interests for: Leif Ristroph\n",
            "Scraped research interests for: Sylvia Serfaty\n",
            "Scraped research interests for: Jalal Shatah\n",
            "Scraped research interests for: Michael Shelley\n",
            "Scraping completed. Results saved to 'research_interests.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Read the CSV file with Google Scholar links\n",
        "df = pd.read_csv('/content/NYU_CIMS_professors_links.csv', header=None, names=['url'])\n",
        "\n",
        "# Function to scrape research interests\n",
        "def scrape_research_interests(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    name = soup.find('div', id='gsc_prf_in').text if soup.find('div', id='gsc_prf_in') else 'N/A'\n",
        "\n",
        "    interests = soup.find('div', id='gsc_prf_int')\n",
        "    if interests:\n",
        "        interests = [i.text for i in interests.find_all('a')]\n",
        "    else:\n",
        "        interests = []\n",
        "\n",
        "    return {\n",
        "        'name': name,\n",
        "        'research_interests': interests,\n",
        "        'url': url\n",
        "    }\n",
        "\n",
        "# List to store scraped data\n",
        "scraped_data = []\n",
        "\n",
        "# Scrape each profile\n",
        "for url in df['url']:\n",
        "    try:\n",
        "        profile_data = scrape_research_interests(url)\n",
        "        scraped_data.append(profile_data)\n",
        "        print(f\"Scraped research interests for: {profile_data['name']}\")\n",
        "        time.sleep(random.uniform(2, 5))  # Random delay between requests\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {str(e)}\")\n",
        "\n",
        "# Create a DataFrame from scraped data\n",
        "result_df = pd.DataFrame(scraped_data)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "result_df.to_csv('research_interests.csv', index=False)\n",
        "print(\"Scraping completed. Results saved to 'research_interests.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65RxDeabTF8Z",
        "outputId": "b037fe4a-8286-4a1c-8f8f-e7a004db652e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraped and saved data for: Scott Armstrong\n",
            "Scraped and saved data for: Marshall Ball\n",
            "Scraped and saved data for: Gerard BEN AROUS\n",
            "Scraped and saved data for: Nir Bitansky\n",
            "Scraped and saved data for: Fedor Bogomolov\n",
            "Scraped and saved data for: Joseph Bonneau\n",
            "Scraped and saved data for: Joan Bruna\n",
            "Scraped and saved data for: Tristan Buckmaster\n",
            "Scraped and saved data for: Yu Chen\n",
            "Scraped and saved data for: Kyunghyun Cho\n",
            "Scraped and saved data for: Eunsol Choi\n",
            "Scraped and saved data for: Sumit Chopra\n",
            "Scraped and saved data for: Richard Cole\n",
            "Scraped and saved data for: Patrick Cousot\n",
            "Scraped and saved data for: David Fouhey\n",
            "Scraped and saved data for: Anupam Gupta\n",
            "Scraped and saved data for: Ernest Davis\n",
            "Scraped and saved data for: percy deift\n",
            "Scraped and saved data for: percy deift\n",
            "Scraped and saved data for: Yevgeniy Dodis\n",
            "Scraped and saved data for: Yevgeniy Dodis\n",
            "Scraped and saved data for: Rob Fergus\n",
            "Scraped and saved data for: Carlos Fernandez-Granda\n",
            "Scraped and saved data for: Gilles Francfort\n",
            "Scraped and saved data for: Alfred Galichon\n",
            "Scraped and saved data for: Davi Geiger\n",
            "Scraped and saved data for: Edwin P. Gerber\n",
            "Scraped and saved data for: Leslie Greengard\n",
            "Scraped and saved data for: Yanjun Han\n",
            "Scraped and saved data for: Arthur Jacot\n",
            "Scraped and saved data for: Qi Lei\n",
            "Scraped and saved data for: Fengbo Hang\n",
            "Scraped and saved data for: Nina Holden\n",
            "Scraped and saved data for: David M Holland\n",
            "Scraped and saved data for: Alan Ali Kaptanoglu\n",
            "Scraped and saved data for: Julia Kempe\n",
            "Scraped and saved data for: Yann LeCun\n",
            "Scraped and saved data for: Chao Li\n",
            "Scraped and saved data for: Jinyang Li\n",
            "Scraped and saved data for: Eyal Lubetzky\n",
            "Scraped and saved data for: Nader Masmoudi\n",
            "Scraped and saved data for: Bud Mishra\n",
            "Scraped and saved data for: Alex Mogilner\n",
            "Scraped and saved data for: Mehryar Mohri\n",
            "Scraped and saved data for: Sunoo Park\n",
            "Scraped and saved data for: Jean Ponce\n",
            "Scraped and saved data for: Michael O'Neil\n",
            "Scraped and saved data for: Michael L. Overton\n",
            "Scraped and saved data for: Aurojit Panda\n",
            "Scraped and saved data for: Daniele Panozzo\n",
            "Scraped and saved data for: Olivier Pauluis\n",
            "Scraped and saved data for: Benjamin Peherstorfer\n",
            "Scraped and saved data for: Ken Perlin\n",
            "Scraped and saved data for: Lerrel Pinto\n",
            "Scraped and saved data for: Jonathan Niles-Weed\n",
            "Scraped and saved data for: Peter Unger\n",
            "Scraped and saved data for: Victor Shoup\n",
            "Scraped and saved data for: Denis Zorin\n",
            "Scraped and saved data for: Yi Zhang（张祎）\n",
            "Scraped and saved data for: Sai Qian Zhang\n",
            "Scraped and saved data for: Jun Zhang, 張駿\n",
            "Scraped and saved data for: Gaoyong Zhang\n",
            "Scraped and saved data for: ofer zeitouni\n",
            "Scraped and saved data for: Laure Zanna\n",
            "Scraped and saved data for: Robert J.C. Young\n",
            "Scraped and saved data for: Chee Yap\n",
            "Scraped and saved data for: Yisong Yang\n",
            "Scraped and saved data for: Sherry Yang\n",
            "Scraped and saved data for: Deane Yang\n",
            "Scraped and saved data for: Hau-Tieng Wu\n",
            "Scraped and saved data for: Saining Xie\n",
            "Scraped and saved data for: Andrew Gordon Wilson\n",
            "Scraped and saved data for: Sam Westrick\n",
            "Scraped and saved data for: Hong Wang\n",
            "Scraped and saved data for: Michael Walfish\n",
            "Scraped and saved data for: Vlad Vicol\n",
            "Scraped and saved data for: Eric Vanden-Eijnden\n",
            "Scraped and saved data for: Matus Telgarsky\n",
            "Scraped and saved data for: Joseph Tassarotti\n",
            "Scraped and saved data for: Rajesh Ranganath\n",
            "Scraped and saved data for: Theodore (Ted) S. Rappaport\n",
            "Scraped and saved data for: Oded Regev\n",
            "Scraped and saved data for: John Rinzel\n",
            "Scraped and saved data for: Leif Ristroph\n",
            "Scraped and saved data for: Sylvia Serfaty\n",
            "Scraped and saved data for: Jalal Shatah\n",
            "Scraped and saved data for: Michael Shelley\n",
            "Scraping completed. Individual professor data saved in 'professor_data' directory.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Read the CSV file with Google Scholar links\n",
        "df = pd.read_csv('/content/NYU_CIMS_professors_links.csv', header=None, names=['url'])\n",
        "\n",
        "# Function to scrape profile data\n",
        "def scrape_profile(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    name = soup.find('div', id='gsc_prf_in').text if soup.find('div', id='gsc_prf_in') else 'N/A'\n",
        "    affiliation = soup.find('div', class_='gsc_prf_il').text if soup.find('div', class_='gsc_prf_il') else 'N/A'\n",
        "\n",
        "    citations = soup.find('td', class_='gsc_rsb_std')\n",
        "    total_citations = citations.text if citations else 'N/A'\n",
        "\n",
        "    h_index = soup.find_all('td', class_='gsc_rsb_std')[2]\n",
        "    h_index = h_index.text if h_index else 'N/A'\n",
        "\n",
        "    interests = soup.find('div', id='gsc_prf_int')\n",
        "    if interests:\n",
        "        interests = [i.text for i in interests.find_all('a')]\n",
        "    else:\n",
        "        interests = []\n",
        "\n",
        "    return {\n",
        "        'name': name,\n",
        "        'affiliation': affiliation,\n",
        "        'total_citations': total_citations,\n",
        "        'h_index': h_index,\n",
        "        'research_interests': interests,\n",
        "        'url': url\n",
        "    }\n",
        "\n",
        "# Create a directory to store individual files\n",
        "os.makedirs('professor_data', exist_ok=True)\n",
        "\n",
        "# Scrape each profile and save to individual files\n",
        "for url in df['url']:\n",
        "    try:\n",
        "        profile_data = scrape_profile(url)\n",
        "\n",
        "        # Create a filename based on the professor's name\n",
        "        filename = f\"professor_data/{profile_data['name'].replace(' ', '_')}.json\"\n",
        "\n",
        "        # Save the data to a JSON file\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(profile_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"Scraped and saved data for: {profile_data['name']}\")\n",
        "        time.sleep(random.uniform(2, 5))  # Random delay between requests\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {str(e)}\")\n",
        "\n",
        "print(\"Scraping completed. Individual professor data saved in 'professor_data' directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR5TCit0USU3",
        "outputId": "72dea05a-0bc6-4d26-c8a6-a2fb2c05ce29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraped and saved data for: Scott Armstrong\n",
            "Scraped and saved data for: Marshall Ball\n",
            "Scraped and saved data for: Gerard BEN AROUS\n",
            "Scraped and saved data for: Nir Bitansky\n",
            "Scraped and saved data for: Fedor Bogomolov\n",
            "Scraped and saved data for: Joseph Bonneau\n",
            "Scraped and saved data for: Joan Bruna\n",
            "Scraped and saved data for: Tristan Buckmaster\n",
            "Error scraping https://scholar.google.com/citations?hl=en&user=CFFEvpEAAAAJ: 'NoneType' object has no attribute 'text'\n",
            "Scraped and saved data for: Kyunghyun Cho\n",
            "Scraped and saved data for: Eunsol Choi\n",
            "Scraped and saved data for: Sumit Chopra\n",
            "Scraped and saved data for: Richard Cole\n",
            "Scraped and saved data for: Patrick Cousot\n",
            "Scraped and saved data for: David Fouhey\n",
            "Scraped and saved data for: Anupam Gupta\n",
            "Scraped and saved data for: Ernest Davis\n",
            "Scraped and saved data for: percy deift\n",
            "Scraped and saved data for: percy deift\n",
            "Scraped and saved data for: Yevgeniy Dodis\n",
            "Scraped and saved data for: Yevgeniy Dodis\n",
            "Scraped and saved data for: Rob Fergus\n",
            "Scraped and saved data for: Carlos Fernandez-Granda\n",
            "Scraped and saved data for: Gilles Francfort\n",
            "Scraped and saved data for: Alfred Galichon\n",
            "Scraped and saved data for: Davi Geiger\n",
            "Scraped and saved data for: Edwin P. Gerber\n",
            "Scraped and saved data for: Leslie Greengard\n",
            "Scraped and saved data for: Yanjun Han\n",
            "Scraped and saved data for: Arthur Jacot\n",
            "Scraped and saved data for: Qi Lei\n",
            "Scraped and saved data for: Fengbo Hang\n",
            "Scraped and saved data for: Nina Holden\n",
            "Scraped and saved data for: David M Holland\n",
            "Scraped and saved data for: Alan Ali Kaptanoglu\n",
            "Scraped and saved data for: Julia Kempe\n",
            "Scraped and saved data for: Yann LeCun\n",
            "Scraped and saved data for: Chao Li\n",
            "Scraped and saved data for: Jinyang Li\n",
            "Scraped and saved data for: Eyal Lubetzky\n",
            "Scraped and saved data for: Nader Masmoudi\n",
            "Scraped and saved data for: Bud Mishra\n",
            "Scraped and saved data for: Alex Mogilner\n",
            "Scraped and saved data for: Mehryar Mohri\n",
            "Scraped and saved data for: Sunoo Park\n",
            "Scraped and saved data for: Jean Ponce\n",
            "Scraped and saved data for: Michael O'Neil\n",
            "Scraped and saved data for: Michael L. Overton\n",
            "Scraped and saved data for: Aurojit Panda\n",
            "Scraped and saved data for: Daniele Panozzo\n",
            "Scraped and saved data for: Olivier Pauluis\n",
            "Scraped and saved data for: Benjamin Peherstorfer\n",
            "Scraped and saved data for: Ken Perlin\n",
            "Scraped and saved data for: Lerrel Pinto\n",
            "Scraped and saved data for: Jonathan Niles-Weed\n",
            "Scraped and saved data for: Peter Unger\n",
            "Scraped and saved data for: Victor Shoup\n",
            "Scraped and saved data for: Denis Zorin\n",
            "Scraped and saved data for: Yi Zhang（张祎）\n",
            "Scraped and saved data for: Sai Qian Zhang\n",
            "Scraped and saved data for: Jun Zhang, 張駿\n",
            "Scraped and saved data for: Gaoyong Zhang\n",
            "Scraped and saved data for: ofer zeitouni\n",
            "Scraped and saved data for: Laure Zanna\n",
            "Scraped and saved data for: Robert J.C. Young\n",
            "Scraped and saved data for: Chee Yap\n",
            "Scraped and saved data for: Yisong Yang\n",
            "Scraped and saved data for: Sherry Yang\n",
            "Scraped and saved data for: Deane Yang\n",
            "Scraped and saved data for: Hau-Tieng Wu\n",
            "Scraped and saved data for: Saining Xie\n",
            "Scraped and saved data for: Andrew Gordon Wilson\n",
            "Scraped and saved data for: Sam Westrick\n",
            "Scraped and saved data for: Hong Wang\n",
            "Scraped and saved data for: Michael Walfish\n",
            "Scraped and saved data for: Vlad Vicol\n",
            "Scraped and saved data for: Eric Vanden-Eijnden\n",
            "Scraped and saved data for: Matus Telgarsky\n",
            "Scraped and saved data for: Joseph Tassarotti\n",
            "Scraped and saved data for: Rajesh Ranganath\n",
            "Scraped and saved data for: Theodore (Ted) S. Rappaport\n",
            "Scraped and saved data for: Oded Regev\n",
            "Scraped and saved data for: John Rinzel\n",
            "Scraped and saved data for: Leif Ristroph\n",
            "Scraped and saved data for: Sylvia Serfaty\n",
            "Scraped and saved data for: Jalal Shatah\n",
            "Scraped and saved data for: Michael Shelley\n",
            "Scraping completed. Individual professor data saved in 'professor_data' directory.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Read the CSV file with Google Scholar links\n",
        "df = pd.read_csv('/content/NYU_CIMS_professors_links.csv', header=None, names=['url'])\n",
        "\n",
        "# Function to scrape profile data and research papers\n",
        "def scrape_profile_and_papers(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    name = soup.find('div', id='gsc_prf_in').text if soup.find('div', id='gsc_prf_in') else 'N/A'\n",
        "    affiliation = soup.find('div', class_='gsc_prf_il').text if soup.find('div', class_='gsc_prf_il') else 'N/A'\n",
        "\n",
        "    # Scrape research papers\n",
        "    papers = []\n",
        "    paper_elements = soup.find_all('tr', class_='gsc_a_tr')\n",
        "    for paper in paper_elements:\n",
        "        title = paper.find('a', class_='gsc_a_at').text\n",
        "        authors = paper.find('div', class_='gs_gray').text\n",
        "        publication = paper.find_all('div', class_='gs_gray')[1].text\n",
        "        year = paper.find('span', class_='gsc_a_h gsc_a_hc gs_ibl').text\n",
        "        citations = paper.find('a', class_='gsc_a_ac gs_ibl').text\n",
        "\n",
        "        papers.append({\n",
        "            'title': title,\n",
        "            'authors': authors,\n",
        "            'publication': publication,\n",
        "            'year': year,\n",
        "            'citations': citations\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'name': name,\n",
        "        'affiliation': affiliation,\n",
        "        'url': url,\n",
        "        'papers': papers\n",
        "    }\n",
        "\n",
        "# Create a directory to store individual files\n",
        "os.makedirs('professor_data', exist_ok=True)\n",
        "\n",
        "# Scrape each profile and save to individual files\n",
        "for url in df['url']:\n",
        "    try:\n",
        "        profile_data = scrape_profile_and_papers(url)\n",
        "\n",
        "        # Create a filename based on the professor's name\n",
        "        filename = f\"professor_data/{profile_data['name'].replace(' ', '_')}.json\"\n",
        "\n",
        "        # Save the data to a JSON file\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(profile_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"Scraped and saved data for: {profile_data['name']}\")\n",
        "        time.sleep(random.uniform(2, 5))  # Random delay between requests\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {str(e)}\")\n",
        "\n",
        "print(\"Scraping completed. Individual professor data saved in 'professor_data' directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-E64WCicfp_V",
        "outputId": "1a942f51-5ee5-42ac-f6f3-14221ce95a64"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_9e60eb9c-2236-49cb-8841-ef920c69606e\", \"content_folder.zip\", 7246181)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Define the source folder and the name of the zip file\n",
        "source_folder = '/content'\n",
        "zip_file_name = 'content_folder.zip'\n",
        "\n",
        "# Create a zip archive of the /content folder\n",
        "shutil.make_archive(zip_file_name.replace('.zip', ''), 'zip', source_folder)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jq3bVADgKOO",
        "outputId": "48616fc2-1311-48b7-f36b-19b67b509d91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sortedcontainers, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [57.7 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,162 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,377 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,654 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,451 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,415 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,601 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [54.5 kB]\n",
            "Fetched 19.2 MB in 2s (8,056 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 29.0 MB of archives.\n",
            "After this operation, 120 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.65.3+22.04 [26.4 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 29.0 MB in 1s (38.4 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123622 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123830 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.65.3+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.65.3+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.65.3+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124059 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n"
          ]
        }
      ],
      "source": [
        "# Install required packages first\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUX3g0TzgYHn",
        "outputId": "663e41d0-791b-4d65-bb9e-0bd853e902e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing professor 1/87\n",
            "Processing professor 2/87\n",
            "Processing professor 3/87\n",
            "Processing professor 4/87\n",
            "Processing professor 5/87\n",
            "Processing professor 6/87\n",
            "Processing professor 7/87\n",
            "Processing professor 8/87\n",
            "Processing professor 9/87\n",
            "Processing professor 10/87\n",
            "Processing professor 11/87\n",
            "Processing professor 12/87\n",
            "Processing professor 13/87\n",
            "Processing professor 14/87\n",
            "Processing professor 15/87\n",
            "Processing professor 16/87\n",
            "Processing professor 17/87\n",
            "Processing professor 18/87\n",
            "Processing professor 19/87\n",
            "Processing professor 20/87\n",
            "Processing professor 21/87\n",
            "Processing professor 22/87\n",
            "Processing professor 23/87\n",
            "Processing professor 24/87\n",
            "Processing professor 25/87\n",
            "Processing professor 26/87\n",
            "Processing professor 27/87\n",
            "Processing professor 28/87\n",
            "Processing professor 29/87\n",
            "Processing professor 30/87\n",
            "Processing professor 31/87\n",
            "Processing professor 32/87\n",
            "Processing professor 33/87\n",
            "Processing professor 34/87\n",
            "Processing professor 35/87\n",
            "Processing professor 36/87\n",
            "Processing professor 37/87\n",
            "Processing professor 38/87\n",
            "Processing professor 39/87\n",
            "Processing professor 40/87\n",
            "Processing professor 41/87\n",
            "Processing professor 42/87\n",
            "Processing professor 43/87\n",
            "Processing professor 44/87\n",
            "Processing professor 45/87\n",
            "Processing professor 46/87\n",
            "Processing professor 47/87\n",
            "Processing professor 48/87\n",
            "Processing professor 49/87\n",
            "Processing professor 50/87\n",
            "Processing professor 51/87\n",
            "Processing professor 52/87\n",
            "Processing professor 53/87\n",
            "Processing professor 54/87\n",
            "Processing professor 55/87\n",
            "Processing professor 56/87\n",
            "Processing professor 57/87\n",
            "Processing professor 58/87\n",
            "Processing professor 59/87\n",
            "Processing professor 60/87\n",
            "Processing professor 61/87\n",
            "Processing professor 62/87\n",
            "Processing professor 63/87\n",
            "Processing professor 64/87\n",
            "Processing professor 65/87\n",
            "Processing professor 66/87\n",
            "Processing professor 67/87\n",
            "Processing professor 68/87\n",
            "Processing professor 69/87\n",
            "Processing professor 70/87\n",
            "Processing professor 71/87\n",
            "Processing professor 72/87\n",
            "Processing professor 73/87\n",
            "Processing professor 74/87\n",
            "Processing professor 75/87\n",
            "Processing professor 76/87\n",
            "Processing professor 77/87\n",
            "Processing professor 78/87\n",
            "Processing professor 79/87\n",
            "Processing professor 80/87\n",
            "Processing professor 81/87\n",
            "Processing professor 82/87\n",
            "Processing professor 83/87\n",
            "Processing professor 84/87\n",
            "Processing professor 85/87\n",
            "Processing professor 86/87\n",
            "Processing professor 87/87\n",
            "Data saved to /content/professor_papers_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "def setup_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    return webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "def scrape_paper_data(url, driver):\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(random.uniform(2, 3))\n",
        "\n",
        "        # Get professor name\n",
        "        name = driver.find_element(By.ID, 'gsc_prf_in').text\n",
        "\n",
        "        # Get research interests\n",
        "        interests = []\n",
        "        interest_elements = driver.find_elements(By.CLASS_NAME, 'gsc_prf_inta')\n",
        "        for element in interest_elements:\n",
        "            interests.append(element.text)\n",
        "\n",
        "        # Get papers\n",
        "        papers = []\n",
        "        paper_elements = driver.find_elements(By.CLASS_NAME, 'gsc_a_tr')\n",
        "\n",
        "        for paper in paper_elements:\n",
        "            try:\n",
        "                title_element = paper.find_element(By.CLASS_NAME, 'gsc_a_at')\n",
        "                title = title_element.text\n",
        "                paper_url = title_element.get_attribute('href')\n",
        "\n",
        "                # Get paper description if available\n",
        "                description = \"Description not available\"\n",
        "                if paper_url:\n",
        "                    driver.get(paper_url)\n",
        "                    time.sleep(random.uniform(1, 2))\n",
        "                    desc_elements = driver.find_elements(By.CLASS_NAME, 'gsh_csp')\n",
        "                    if desc_elements:\n",
        "                        description = desc_elements[0].text\n",
        "\n",
        "                papers.append({\n",
        "                    'title': title,\n",
        "                    'description': description\n",
        "                })\n",
        "\n",
        "                # Go back to profile page\n",
        "                driver.back()\n",
        "                time.sleep(random.uniform(1, 2))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing paper: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return {\n",
        "            'name': name,\n",
        "            'research_interests': interests,\n",
        "            'papers': papers\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping profile: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    # Read professor links\n",
        "    df = pd.read_csv('/content/NYU_CIMS_professors_links.csv', header=None, names=['url'])\n",
        "\n",
        "    # Setup output file\n",
        "    output_file = '/content/professor_papers_data.csv'\n",
        "\n",
        "    # Initialize webdriver\n",
        "    driver = setup_driver()\n",
        "\n",
        "    # Process each professor\n",
        "    all_data = []\n",
        "    for index, row in df.iterrows():\n",
        "        print(f\"Processing professor {index + 1}/{len(df)}\")\n",
        "        data = scrape_paper_data(row['url'], driver)\n",
        "        if data:\n",
        "            for paper in data['papers']:\n",
        "                all_data.append({\n",
        "                    'Professor Name': data['name'],\n",
        "                    'Professor Link': row['url'],\n",
        "                    'Research Interests': ', '.join(data['research_interests']),\n",
        "                    'Paper Name': paper['title'],\n",
        "                    'Description': paper['description']\n",
        "                })\n",
        "        time.sleep(random.uniform(3, 5))\n",
        "\n",
        "    # Save to CSV\n",
        "    pd.DataFrame(all_data).to_csv(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "    # Close driver\n",
        "    driver.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UNQ_5sLaKrgd",
        "outputId": "da271aec-10dc-413a-e276-c0ff6b0a067b"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[Errno 28] No space left on device",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1788\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mfdst_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1d827efd4baf>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create a zip archive of the /content folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Download the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_cwd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36m_make_zipfile\u001b[0;34m(base_name, base_dir, verbose, dry_run, logger, owner, group, root_dir)\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                         \u001b[0marcname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marcdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m                         \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adding '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1786\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1788\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1789\u001b[0m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Define the source folder and the name of the zip file\n",
        "source_folder = '/content'\n",
        "zip_file_name = 'NYU_CIMS_content_folder.zip'\n",
        "\n",
        "# Create a zip archive of the /content folder\n",
        "shutil.make_archive(zip_file_name.replace('.zip', ''), 'zip', source_folder)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_file_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}